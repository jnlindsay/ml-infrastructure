{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "- Batch size: $n$\n",
    "- Input size: $d$\n",
    "- Input: `X`, $X_t \\in \\mathbb R^{n \\times d}$\n",
    "  - Thus a \"batch\" at time $t$ comprises training examples $\\mathbf x_1, \\mathbf x_2, \\ldots, \\mathbf x_n \\in \\mathbb R^{1 \\times d}$ packaged as *row vectors* into an $n \\times d$ matrix like so:\n",
    "    $$\n",
    "        X_t \\coloneqq\n",
    "            \\begin{pmatrix}\n",
    "                \\mathbf x_1 \\\\ \\mathbf x_2 \\\\ \\vdots \\\\ \\mathbf x_n\n",
    "            \\end{pmatrix}\n",
    "    $$\n",
    "- Hidden state: `H`, $H_{t - 1} \\in \\mathbb R^{n \\times h}$\n",
    "- Forget gate: `F`, $F_t \\in \\mathbb R^{n \\times h}$\n",
    "- Input gate: `I`, $I_t \\in \\mathbb R^{n \\times h}$\n",
    "- Input node: `C_tilde`, $\\tilde C_t \\in \\mathbb R^{n \\times h}$\n",
    "- Output gate: `O`, $O_t \\in \\mathbb R^{n \\times h}$\n",
    "\n",
    "Define an affine map\n",
    "\\begin{align*}\n",
    "    \\mathrm{Aff}_{W_i, \\mathbf b} \\coloneqq \\mathrm{Aff} : \\mathbb R^{n \\times d} \\times \\cdots \\times \\mathbb R^{n \\times d}\n",
    "        &\\to \\mathbb R^{n \\times h}\n",
    "\\\\\n",
    "    X_1, \\ldots, X_\\ell &\\mapsto \\sum_{i = 1}^\\ell X_i W_i \\oplus \\mathbf b\n",
    "\\end{align*}\n",
    "where $W_i \\in \\mathbb R^{d \\times h}$ and $\\mathbf b \\in \\mathbb R^{1 \\times h}$ are weights and biases, and $\\oplus$ denotes row-wise addition.\n",
    "\n",
    "The gates and input node are calculated thus:\n",
    "\\begin{align*}\n",
    "    F_t &\\coloneqq (\\sigma \\circ \\mathrm{Aff})(X_t, H_{t - 1}) \\\\\n",
    "    I_t &\\coloneqq (\\sigma \\circ \\mathrm{Aff})(X_t, H_{t - 1}) \\\\\n",
    "    O_t &\\coloneqq (\\sigma \\circ \\mathrm{Aff})(X_t, H_{t - 1}) \\\\\n",
    "    \\tilde C_t &\\coloneqq (\\tanh \\circ \\mathrm{Aff})(X_t, H_{t - 1}) .\n",
    "\\end{align*}\n",
    "Here $\\sigma : \\mathbb R^{n \\times h} \\to \\mathbb R^{n \\times h}$ is the activation function (either $\\mathrm{sigmoid}$ or $\\mathrm{ReLU}$) applied component-wise. Finally, the two outputs are computed as\n",
    "\\begin{align*}\n",
    "    C_t &\\coloneqq F_t \\odot C_{t - 1} + I_t \\odot \\tilde C_t\n",
    "\\\\\n",
    "    H_t &\\coloneqq O_t \\odot \\tanh(C_t) ,\n",
    "\\end{align*}\n",
    "where $\\odot$ is the Hadamard product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, sigma=0.01):\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        init_weight = lambda *shape: nn.Parameter(torch.randn(*shape) * sigma)\n",
    "\n",
    "        triple = lambda: (\n",
    "            init_weight(input_size, hidden_size),\n",
    "            init_weight(hidden_size, hidden_size),\n",
    "            nn.Parameter(torch.zeros(hidden_size))\n",
    "        )\n",
    "\n",
    "        self.W_xi, self.W_hi, self.b_i = triple()  # input gate\n",
    "        self.W_xf, self.W_hf, self.b_f = triple()  # forget gate\n",
    "        self.W_xo, self.W_ho, self.b_o = triple()  # output gate\n",
    "        self.W_xc, self.W_hc, self.b_c = triple()  # input node\n",
    "\n",
    "    def forward(self, inputs, H_C=None):\n",
    "        if H_C is None:\n",
    "            # initial state with shape: (batch_size, hidden_size)\n",
    "            H = torch.zeros((inputs.shape[1], self.hidden_size),\n",
    "                device=inputs.device)\n",
    "            C = torch.zeros((inputs.shape[1], self.hidden_size),\n",
    "                device=inputs.device)\n",
    "        \n",
    "        else:\n",
    "            H, C = H_C\n",
    "        \n",
    "        outputs = []\n",
    "        for X in inputs:\n",
    "            I = torch.sigmoid(\n",
    "                torch.matmul(X, self.W_xi) +\n",
    "                torch.matmul(H, self.W_hi) + self.b_i\n",
    "            )\n",
    "            F = torch.sigmoid(\n",
    "                torch.matmul(X, self.W_xf) +\n",
    "                torch.matmul(H, self.W_hf) + self.b_f\n",
    "            )\n",
    "            O = torch.sigmoid(\n",
    "                torch.matmul(X, self.W_xo) +\n",
    "                torch.matmul(H, self.W_ho) + self.b_o\n",
    "            )\n",
    "            C_tilde = torch.tanh(\n",
    "                torch.matmul(X, self.W_xc) +\n",
    "                torch.matmul(H, self.W_hc) + self.b_c\n",
    "            )\n",
    "\n",
    "            C = F * C + I * C_tilde\n",
    "            H = O * torch.tanh(C)\n",
    "\n",
    "            outputs.append(H)\n",
    "\n",
    "        return outputs, (H, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data for long-range dependency\n",
    "def generate_data(num_samples, sequence_length, input_size, threshold):\n",
    "    X = torch.randn(num_samples, sequence_length, input_size)\n",
    "    y = ((X[:, 0, 0] + X[:, -1, 0]) > threshold).long()  # Label based on first and last element\n",
    "    return X, y\n",
    "\n",
    "# Hyperparameters\n",
    "num_samples = 1000\n",
    "sequence_length = 10  # Long sequence\n",
    "input_size = 1\n",
    "hidden_size = 16\n",
    "output_size = 2  # Binary classification\n",
    "batch_size = 32\n",
    "num_epochs = 100\n",
    "threshold = 0.0\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Create dataset\n",
    "X, y = generate_data(num_samples, sequence_length, input_size, threshold)\n",
    "\n",
    "# Split into training and test sets\n",
    "train_size = int(0.8 * num_samples)\n",
    "test_size = num_samples - train_size\n",
    "\n",
    "train_X, test_X = X[:train_size], X[train_size:]\n",
    "train_y, test_y = y[:train_size], y[train_size:]\n",
    "\n",
    "# Data loaders\n",
    "train_data = torch.utils.data.TensorDataset(train_X, train_y)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_data = torch.utils.data.TensorDataset(test_X, test_y)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Fully connected classification head\n",
    "class SequenceClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SequenceClassifier, self).__init__()\n",
    "        self.lstm = LSTM(input_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs, _ = self.lstm(x.permute(1, 0, 2))  # seq_len, batch, input_size\n",
    "        last_output = outputs[-1]  # Use the last hidden state\n",
    "        return self.fc(last_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.690939257144928\n",
      "Epoch 10, Loss: 0.5118549025058746\n",
      "Epoch 20, Loss: 0.4067698395252228\n",
      "Epoch 30, Loss: 0.27837460935115815\n",
      "Epoch 40, Loss: 0.2187814524769783\n",
      "Epoch 50, Loss: 0.1719493129849434\n",
      "Epoch 60, Loss: 0.15739808827638627\n",
      "Epoch 70, Loss: 0.13721859976649284\n",
      "Epoch 80, Loss: 0.13250648885965347\n",
      "Epoch 90, Loss: 0.11599033936858177\n",
      "Test Accuracy: 94.50%\n"
     ]
    }
   ],
   "source": [
    "# Training setup\n",
    "model = SequenceClassifier(input_size, hidden_size, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    if (epoch % 10) == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        outputs = model(batch_X)\n",
    "        predictions = outputs.argmax(dim=1)\n",
    "        correct += (predictions == batch_y).sum().item()\n",
    "        total += batch_y.size(0)\n",
    "\n",
    "print(f\"Test Accuracy: {correct / total:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequences, model predictions, and expected outputs:\n",
      "Sequence 1:\n",
      "  Input: [ 1.3026822  -0.91571444  0.59605736 -0.6351513  -0.04250909  2.0221457\n",
      "  1.2990092  -0.54431796  1.2314717  -2.2671065 ]\n",
      "  Prediction: 1\n",
      "  Expected: 0\n",
      "Sequence 2:\n",
      "  Input: [ 0.9253716  -1.5769877  -0.3064221   1.0338665  -2.0728173   1.443539\n",
      "  0.71105564 -0.6184454  -0.23085828  0.9072259 ]\n",
      "  Prediction: 1\n",
      "  Expected: 1\n",
      "Sequence 3:\n",
      "  Input: [-1.3915073  -0.48210853  0.35327092  0.44365886 -0.76442474  0.32325813\n",
      " -0.12421709 -0.25350264  0.14209668  1.8866785 ]\n",
      "  Prediction: 1\n",
      "  Expected: 1\n",
      "Sequence 4:\n",
      "  Input: [ 2.2446215  -0.17471376  0.6772357  -1.3377231  -1.3301219  -0.28191957\n",
      " -0.20286323  1.2673936   0.65420157 -1.82388   ]\n",
      "  Prediction: 1\n",
      "  Expected: 1\n",
      "Sequence 5:\n",
      "  Input: [ 0.6712648  -1.0581725  -0.56995696  0.16180553  0.556193    0.5367684\n",
      "  0.74680084  0.4026059  -0.3619496  -0.07254669]\n",
      "  Prediction: 1\n",
      "  Expected: 1\n"
     ]
    }
   ],
   "source": [
    "# Example Test: Print inputs, predictions, and expected outputs\n",
    "model.eval()\n",
    "\n",
    "# Select a few samples from the test set\n",
    "example_X = test_X[:5]\n",
    "example_y = test_y[:5]\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(example_X)\n",
    "    predictions = outputs.argmax(dim=1)\n",
    "\n",
    "print(\"Input sequences, model predictions, and expected outputs:\")\n",
    "for i in range(len(example_X)):\n",
    "    print(f\"Sequence {i + 1}:\")\n",
    "    print(f\"  Input: {example_X[i].squeeze(-1).numpy()}\")\n",
    "    print(f\"  Prediction: {predictions[i].item()}\")\n",
    "    print(f\"  Expected: {example_y[i].item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
