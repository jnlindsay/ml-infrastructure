{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "(*Please visit [this link](https://d2l.ai/chapter_recurrent-modern/lstm.html) to learn more about LSTMs.*)\n",
    "\n",
    "The following diagram represents the LSTM implemented in this notebook. The symbols resembling a widened Roman numeral \"II\" represent gates.\n",
    "\n",
    "![something](../img/lstm-diagram-handdrawn.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical details\n",
    "\n",
    "Notation\n",
    "- Batch size: $n$ as `n`\n",
    "- Input size: $d$ as `d`\n",
    "- Input: $X_t \\in \\mathbb R^{n \\times d}$ as `X`\n",
    "  - Thus a \"batch\" at time $t$ comprises training examples $\\mathbf x_1, \\mathbf x_2, \\ldots, \\mathbf x_n \\in \\mathbb R^{1 \\times d}$ packaged as *row vectors* into an $n \\times d$ matrix like so:\n",
    "    $$\n",
    "        X_t \\coloneqq\n",
    "            \\begin{pmatrix}\n",
    "                \\mathbf x_1 \\\\ \\mathbf x_2 \\\\ \\vdots \\\\ \\mathbf x_n\n",
    "            \\end{pmatrix}\n",
    "    $$\n",
    "- Hidden state: $H_{t - 1} \\in \\mathbb R^{n \\times h}$ as `H`\n",
    "- Forget gate: $F_t \\in \\mathbb R^{n \\times h}$ as `F`\n",
    "- Input gate: $I_t \\in \\mathbb R^{n \\times h}$ as `I`\n",
    "- Input node: $\\tilde C_t \\in \\mathbb R^{n \\times h}$ as `C_tilde`\n",
    "- Output gate: $O_t \\in \\mathbb R^{n \\times h}$ as `O`\n",
    "\n",
    "Define an affine map\n",
    "\\begin{align*}\n",
    "    \\mathrm{Aff} \\coloneqq \\mathrm{Aff}_{W_i, \\mathbf b} : \\mathbb R^{n \\times d} \\times \\cdots \\times \\mathbb R^{n \\times d}\n",
    "        &\\to \\mathbb R^{n \\times h}\n",
    "\\\\\n",
    "    X_1, \\ldots, X_\\ell &\\mapsto \\sum_{i = 1}^\\ell X_i W_i \\oplus \\mathbf b\n",
    "\\end{align*}\n",
    "where $W_i \\in \\mathbb R^{d \\times h}$ and $\\mathbf b \\in \\mathbb R^{1 \\times h}$ are weights and biases, and $\\oplus$ denotes row-wise addition.\n",
    "\n",
    "The gates and input node are calculated thus:\n",
    "\\begin{align*}\n",
    "    F_t &\\coloneqq (\\sigma \\circ \\mathrm{Aff})(X_t, H_{t - 1}) \\\\\n",
    "    I_t &\\coloneqq (\\sigma \\circ \\mathrm{Aff})(X_t, H_{t - 1}) \\\\\n",
    "    O_t &\\coloneqq (\\sigma \\circ \\mathrm{Aff})(X_t, H_{t - 1}) \\\\\n",
    "    \\tilde C_t &\\coloneqq (\\tanh \\circ \\mathrm{Aff})(X_t, H_{t - 1}) .\n",
    "\\end{align*}\n",
    "Here $\\tanh$ is applied component-wise, and $\\sigma : \\mathbb R^{n \\times h} \\to \\mathbb R^{n \\times h}$ is the activation function (either $\\mathrm{sigmoid}$ or $\\mathrm{ReLU}$) applied component-wise. Finally, the two outputs are computed as\n",
    "\\begin{align*}\n",
    "    C_t &\\coloneqq F_t \\odot C_{t - 1} + I_t \\odot \\tilde C_t\n",
    "\\\\\n",
    "    H_t &\\coloneqq O_t \\odot \\tanh(C_t) ,\n",
    "\\end{align*}\n",
    "where $\\odot$ is the Hadamard product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, sigma=0.01):\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        init_weight = lambda *shape: nn.Parameter(torch.randn(*shape) * sigma)\n",
    "\n",
    "        triple = lambda: (\n",
    "            init_weight(input_size, hidden_size),\n",
    "            init_weight(hidden_size, hidden_size),\n",
    "            nn.Parameter(torch.zeros(hidden_size))\n",
    "        )\n",
    "\n",
    "        self.W_xi, self.W_hi, self.b_i = triple()  # input gate\n",
    "        self.W_xf, self.W_hf, self.b_f = triple()  # forget gate\n",
    "        self.W_xo, self.W_ho, self.b_o = triple()  # output gate\n",
    "        self.W_xc, self.W_hc, self.b_c = triple()  # input node\n",
    "\n",
    "    def forward(self, inputs, H_C=None):\n",
    "        if H_C is None:\n",
    "            # initial state with shape: (batch_size, hidden_size)\n",
    "            H = torch.zeros((inputs.shape[1], self.hidden_size),\n",
    "                device=inputs.device)\n",
    "            C = torch.zeros((inputs.shape[1], self.hidden_size),\n",
    "                device=inputs.device)\n",
    "        \n",
    "        else:\n",
    "            H, C = H_C\n",
    "        \n",
    "        outputs = []\n",
    "        for X in inputs:\n",
    "            I = torch.sigmoid(\n",
    "                torch.matmul(X, self.W_xi) +\n",
    "                torch.matmul(H, self.W_hi) + self.b_i\n",
    "            )\n",
    "            F = torch.sigmoid(\n",
    "                torch.matmul(X, self.W_xf) +\n",
    "                torch.matmul(H, self.W_hf) + self.b_f\n",
    "            )\n",
    "            O = torch.sigmoid(\n",
    "                torch.matmul(X, self.W_xo) +\n",
    "                torch.matmul(H, self.W_ho) + self.b_o\n",
    "            )\n",
    "            C_tilde = torch.tanh(\n",
    "                torch.matmul(X, self.W_xc) +\n",
    "                torch.matmul(H, self.W_hc) + self.b_c\n",
    "            )\n",
    "\n",
    "            C = F * C + I * C_tilde\n",
    "            H = O * torch.tanh(C)\n",
    "\n",
    "            outputs.append(H)\n",
    "\n",
    "        return outputs, (H, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data for long-range dependency\n",
    "def generate_data(num_samples, sequence_length, input_size, threshold):\n",
    "    X = torch.randn(num_samples, sequence_length, input_size)\n",
    "    y = ((X[:, 0, 0] + X[:, -1, 0]) > threshold).long()  # Label based on first and last element\n",
    "    return X, y\n",
    "\n",
    "# Hyperparameters\n",
    "num_samples = 1000\n",
    "sequence_length = 10  # Long sequence\n",
    "input_size = 1\n",
    "hidden_size = 16\n",
    "output_size = 2  # Binary classification\n",
    "batch_size = 32\n",
    "num_epochs = 100\n",
    "threshold = 0.0\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Create dataset\n",
    "X, y = generate_data(num_samples, sequence_length, input_size, threshold)\n",
    "\n",
    "# Split into training and test sets\n",
    "train_size = int(0.8 * num_samples)\n",
    "test_size = num_samples - train_size\n",
    "\n",
    "train_X, test_X = X[:train_size], X[train_size:]\n",
    "train_y, test_y = y[:train_size], y[train_size:]\n",
    "\n",
    "# Data loaders\n",
    "train_data = torch.utils.data.TensorDataset(train_X, train_y)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_data = torch.utils.data.TensorDataset(test_X, test_y)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Fully connected classification head\n",
    "class SequenceClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SequenceClassifier, self).__init__()\n",
    "        self.lstm = LSTM(input_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs, _ = self.lstm(x.permute(1, 0, 2))  # seq_len, batch, input_size\n",
    "        last_output = outputs[-1]  # Use the last hidden state\n",
    "        return self.fc(last_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.6909745097160339\n",
      "Epoch 10, Loss: 0.480657662153244\n",
      "Epoch 20, Loss: 0.4222220098972321\n",
      "Epoch 30, Loss: 0.330008819103241\n",
      "Epoch 40, Loss: 0.24581120789051056\n",
      "Epoch 50, Loss: 0.19835255473852156\n",
      "Epoch 60, Loss: 0.17425710171461106\n",
      "Epoch 70, Loss: 0.1433921954035759\n",
      "Epoch 80, Loss: 0.13547056362032892\n",
      "Epoch 90, Loss: 0.12959731206297875\n",
      "Test Accuracy: 92.00%\n"
     ]
    }
   ],
   "source": [
    "# Training setup\n",
    "model = SequenceClassifier(input_size, hidden_size, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    if (epoch % 10) == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        outputs = model(batch_X)\n",
    "        predictions = outputs.argmax(dim=1)\n",
    "        correct += (predictions == batch_y).sum().item()\n",
    "        total += batch_y.size(0)\n",
    "\n",
    "print(f\"Test Accuracy: {correct / total:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequences, model predictions, and expected outputs:\n",
      "Sequence 1:\n",
      "  Input: [-1.712781    0.15639406  0.400885    1.022108    0.7323962   0.2487995\n",
      " -0.37633613 -1.6895779   0.27961224 -2.5285008 ]\n",
      "  Prediction: 0\n",
      "  Expected: 0\n",
      "Sequence 2:\n",
      "  Input: [ 1.6508069   0.7854169  -0.56765074  0.20695522  0.9156484  -0.13021418\n",
      "  1.1815516   1.0346842   0.16539836  0.13768348]\n",
      "  Prediction: 1\n",
      "  Expected: 1\n",
      "Sequence 3:\n",
      "  Input: [-0.21803427  1.8969357  -0.5764769   0.3667038  -0.60600954  0.71313083\n",
      "  0.11953576 -0.09205437  0.9492798  -0.89113367]\n",
      "  Prediction: 0\n",
      "  Expected: 0\n",
      "Sequence 4:\n",
      "  Input: [-0.10421421 -0.21582405 -3.1003687   0.5999893  -1.8100554  -0.59494805\n",
      "  0.84108335  0.11350693 -0.5673124  -0.65795314]\n",
      "  Prediction: 0\n",
      "  Expected: 0\n",
      "Sequence 5:\n",
      "  Input: [-0.41653213  0.368179   -0.344845    1.928492    0.8832354   0.5561767\n",
      " -0.38123277 -1.2039412  -0.75858384 -0.36167514]\n",
      "  Prediction: 0\n",
      "  Expected: 0\n"
     ]
    }
   ],
   "source": [
    "# Example Test: Print inputs, predictions, and expected outputs\n",
    "model.eval()\n",
    "\n",
    "# Select a few samples from the test set\n",
    "example_X = test_X[:5]\n",
    "example_y = test_y[:5]\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(example_X)\n",
    "    predictions = outputs.argmax(dim=1)\n",
    "\n",
    "print(\"Input sequences, model predictions, and expected outputs:\")\n",
    "for i in range(len(example_X)):\n",
    "    print(f\"Sequence {i + 1}:\")\n",
    "    print(f\"  Input: {example_X[i].squeeze(-1).numpy()}\")\n",
    "    print(f\"  Prediction: {predictions[i].item()}\")\n",
    "    print(f\"  Expected: {example_y[i].item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
