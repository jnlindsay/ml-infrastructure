{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, sigma=0.01):\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        init_weight = lambda *shape: nn.Parameter(torch.randn(*shape) * sigma)\n",
    "        triple = lambda: (init_weight(input_size, hidden_size),\n",
    "                         init_weight(hidden_size, hidden_size),\n",
    "                         nn.Parameter(torch.zeros(hidden_size)))\n",
    "\n",
    "        self.W_xi, self.W_hi, self.b_i = triple()  # input gate\n",
    "        self.W_xf, self.W_hf, self.b_f = triple()  # forget gate\n",
    "        self.W_xo, self.W_ho, self.b_o = triple()  # output gate\n",
    "        self.W_xc, self.W_hc, self.b_c = triple()  # input node\n",
    "\n",
    "    def forward(self, inputs, H_C=None):\n",
    "        if H_C is None:\n",
    "            # initial state with shape: (batch_size, hidden_size)\n",
    "            H = torch.zeros((inputs.shape[1], self.hidden_size),\n",
    "                device=inputs.device)\n",
    "            C = torch.zeros((inputs.shape[1], self.hidden_size),\n",
    "                device=inputs.device)\n",
    "        \n",
    "        else:\n",
    "            H, C = H_C\n",
    "        \n",
    "        outputs = []\n",
    "        for X in inputs:\n",
    "            I = torch.sigmoid(\n",
    "                torch.matmul(X, self.W_xi) +\n",
    "                torch.matmul(H, self.W_hi) + self.b_i\n",
    "            )\n",
    "            F = torch.sigmoid(\n",
    "                torch.matmul(X, self.W_xf) +\n",
    "                torch.matmul(H, self.W_hf) + self.b_f\n",
    "            )\n",
    "            O = torch.sigmoid(\n",
    "                torch.matmul(X, self.W_xo) +\n",
    "                torch.matmul(H, self.W_ho) + self.b_o\n",
    "            )\n",
    "            C_tilde = torch.tanh(\n",
    "                torch.matmul(X, self.W_xc) +\n",
    "                torch.matmul(H, self.W_hc) + self.b_c\n",
    "            )\n",
    "\n",
    "            C = F * C + I * C_tilde\n",
    "            H = O * torch.tanh(C)\n",
    "\n",
    "            outputs.append(H)\n",
    "\n",
    "        return outputs, (H, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data for long-range dependency\n",
    "def generate_data(num_samples, sequence_length, input_size, threshold):\n",
    "    X = torch.randn(num_samples, sequence_length, input_size)\n",
    "    y = ((X[:, 0, 0] + X[:, -1, 0]) > threshold).long()  # Label based on first and last element\n",
    "    return X, y\n",
    "\n",
    "# Hyperparameters\n",
    "num_samples = 1000\n",
    "sequence_length = 10  # Long sequence\n",
    "input_size = 1\n",
    "hidden_size = 16\n",
    "output_size = 2  # Binary classification\n",
    "batch_size = 32\n",
    "num_epochs = 100\n",
    "threshold = 0.0\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Create dataset\n",
    "X, y = generate_data(num_samples, sequence_length, input_size, threshold)\n",
    "\n",
    "# Split into training and test sets\n",
    "train_size = int(0.8 * num_samples)\n",
    "test_size = num_samples - train_size\n",
    "\n",
    "train_X, test_X = X[:train_size], X[train_size:]\n",
    "train_y, test_y = y[:train_size], y[train_size:]\n",
    "\n",
    "# Data loaders\n",
    "train_data = torch.utils.data.TensorDataset(train_X, train_y)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_data = torch.utils.data.TensorDataset(test_X, test_y)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Fully connected classification head\n",
    "class SequenceClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SequenceClassifier, self).__init__()\n",
    "        self.lstm = LSTM(input_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs, _ = self.lstm(x.permute(1, 0, 2))  # seq_len, batch, input_size\n",
    "        last_output = outputs[-1]  # Use the last hidden state\n",
    "        return self.fc(last_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.6944447898864746\n",
      "Epoch 10, Loss: 0.557159765958786\n",
      "Epoch 20, Loss: 0.48266212940216063\n",
      "Epoch 30, Loss: 0.47914249300956724\n",
      "Epoch 40, Loss: 0.47136218309402467\n",
      "Epoch 50, Loss: 0.36938887476921084\n",
      "Epoch 60, Loss: 0.2578399443626404\n",
      "Epoch 70, Loss: 0.22192418575286865\n",
      "Epoch 80, Loss: 0.1905925652384758\n",
      "Epoch 90, Loss: 0.1663687564432621\n",
      "Test Accuracy: 96.00%\n"
     ]
    }
   ],
   "source": [
    "# Training setup\n",
    "model = SequenceClassifier(input_size, hidden_size, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    if (epoch % 10) == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        outputs = model(batch_X)\n",
    "        predictions = outputs.argmax(dim=1)\n",
    "        correct += (predictions == batch_y).sum().item()\n",
    "        total += batch_y.size(0)\n",
    "\n",
    "print(f\"Test Accuracy: {correct / total:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequences, model predictions, and expected outputs:\n",
      "Sequence 1:\n",
      "  Input: [-0.20597954 -1.198035    1.6662222  -0.859738    0.47962168 -0.12167113\n",
      "  1.0808519   0.35500762  1.585527   -0.42019925]\n",
      "  Prediction: 0\n",
      "  Expected: 0\n",
      "Sequence 2:\n",
      "  Input: [-0.6446867  -1.4907006   0.9217508   0.30053535 -1.6256307   0.1004274\n",
      "  0.23721616 -1.6597656   0.6699194  -0.01552773]\n",
      "  Prediction: 0\n",
      "  Expected: 0\n",
      "Sequence 3:\n",
      "  Input: [-0.8113887   0.09116855 -0.69002235  0.7237788  -1.1907287  -0.9923395\n",
      " -0.108426   -0.22399668 -0.79839045 -1.5641514 ]\n",
      "  Prediction: 0\n",
      "  Expected: 0\n",
      "Sequence 4:\n",
      "  Input: [ 1.2063242   0.29052877 -0.26716873  0.7285104  -0.14099081  0.5734504\n",
      "  0.18021065 -0.43092886 -0.7307493  -0.17880793]\n",
      "  Prediction: 1\n",
      "  Expected: 1\n",
      "Sequence 5:\n",
      "  Input: [ 0.3999417   0.59730524  2.2906618   1.2805545   1.1404402   0.32964057\n",
      " -0.96988887 -2.097572   -0.145355    0.74130315]\n",
      "  Prediction: 1\n",
      "  Expected: 1\n"
     ]
    }
   ],
   "source": [
    "# Example Test: Print inputs, predictions, and expected outputs\n",
    "model.eval()\n",
    "\n",
    "# Select a few samples from the test set\n",
    "example_X = test_X[:5]\n",
    "example_y = test_y[:5]\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(example_X)\n",
    "    predictions = outputs.argmax(dim=1)\n",
    "\n",
    "print(\"Input sequences, model predictions, and expected outputs:\")\n",
    "for i in range(len(example_X)):\n",
    "    print(f\"Sequence {i + 1}:\")\n",
    "    print(f\"  Input: {example_X[i].squeeze(-1).numpy()}\")\n",
    "    print(f\"  Prediction: {predictions[i].item()}\")\n",
    "    print(f\"  Expected: {example_y[i].item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
